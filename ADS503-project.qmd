---
title: "ADS503 Project"
group: "#1"
names: "Jason Avalos, Lindy Conrad, Duy-Anh Dang"
---
## Libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(caret)
library(pROC)
library(randomForest)
library(e1071)
library(glmnet)
library(corrplot)
```

## Load Data

```{r}
# Loads Sirtuin6 Small Molecules Dataset: The dataset includes 100 molecules with 6 most relevant descriptors to determine the candidate inhibitors of a target protein, Sirtuin6. The molecules are grouped based on their low- and high-BFEs.
data <- read.csv(
  file = "./SIRTUIN6.csv",
  header = TRUE,
  stringsAsFactors = FALSE
)

head(data)
```
## DA (graphical and non-graphical representations of relationships between the response variable and predictor variables)

```{r}
summary(data)
```
```{r}
table(data$Class)
```


```{r}
predictors <- setdiff(names(data), "Class")

# Loop through each predictor and print each histogram
for (var in predictors) {
  p <- ggplot(data, aes_string(x = var)) +
    geom_histogram(bins = 25) +
    labs(
      title = paste("Histogram of", var),
      x     = var,
      y     = "Count"
    ) +
    theme_minimal()
  
  print(p)
}

# Boxplots to explore distribution, central tendency and outliers.
boxplot(scale(select(data, -Class)))
```

```{r}
#Correlation Matrix
corr_mat <- cor(select(data, -Class))
corrplot(corr_mat, 
         method="color", 
         tl.cex=0.7,
         addCoef.col = "white",
         type = "lower", 
         )
title(main="Correlations between predictors")
```

## Data wrangling and pre-processing (handling of missing values, outliers, correlated features, etc.)

```{r}
# Center & scale all predictors
preProc <- preProcess(select(data, -Class), method = c("center","scale"))
data_pp <- predict(preProc, select(data, -Class))
data_pp$Class <- data$Class
```

## Data splitting (training, validation, and test sets)
```{r}
#80/20 Train Test Set Split
set.seed(123)
trainIndex <- createDataPartition(data_pp$Class, p = 0.8, list = FALSE)
train <- data_pp[trainIndex, ]
test  <- data_pp[-trainIndex, ]
table(train$Class); table(test$Class)
```


```{r}
ctrl <- trainControl(
  method = "cv", 
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE
)

#Models

#Logistic Regression 
set.seed(123)
lrFit <- train(
  Class ~ ., data = train,
  method = "glm",
  metric = "ROC",
  trControl = ctrl
)

lrFit

# Random Forest 
set.seed(123)
rfGrid <- expand.grid(mtry = 2:15)
rfFit <- train(
  Class ~ ., data = train,
  method=  "rf",
  metric = "ROC",
  trControl= ctrl,
  tuneGrid = rfGrid,
  ntree = 500
)

rfFit

#SVM RBF
set.seed(123)
svmGrid <- expand.grid(
  C = c(0.1, 1, 10),
  sigma = c(0.01, 0.05, 0.1)
)
svmFit <- train(
  Class ~ ., data = train,
  method = "svmRadial",
  metric = "ROC",
  trControl= ctrl,
  tuneGrid = svmGrid
)

svmFit

# Lasso
lassoGrid <- expand.grid(
  alpha = 1,
  lambda = seq(0.0001, 1, length = 20)
)

set.seed(123)
lassoFit <- train(
 Class ~ ., data = train,
  method = "glmnet",
  tuneGrid = lassoGrid,
  preProc = c("center", "scale"),
  metric = "ROC",
  trControl = ctrl
)

lassoFit
```
## Validation and testing (model tuning and evaluation)

```{r}
# Hyperparameter Tuning 
lrFit$bestTune
rfFit$bestTune
svmFit$bestTune
lassoFit$bestTune
```
```{r}
# Hyperparameter plots
plot(rfFit)
plot(svmFit)
plot(lassoFit)
```
```{r}
# Model performance metrics comparison
resamps <- resamples(list(
  Logistic = lrFit,
  randomForest = rfFit,
  SVM = svmFit,
  Lasso = lassoFit
))

summary(resamps)
```

## Results and final model selection (performance measures, etc.)
```{r}
# add final model info here
lrFitImp <- varImp(lrFit, scale = FALSE) #update
plot(lrFitImp)
```
```{r}
# Used the following two lines to ick the positive class
# prob_cols <- colnames(predict(lrFit, test, type = "prob"))
# print(prob_cols)
positive <- "High_BFE"
negative <- setdiff(levels(test$Class), positive)

# Compute ROCs objects and 95% CIs
roc_lr <- pROC::roc(response  = test$Class, predictor = predict(lrFit,    test, type = "prob")[, positive])
ci_lr <- pROC::ci.auc(roc_lr)

roc_rf <- pROC::roc(response  = test$Class, predictor = predict(rfFit,    test, type = "prob")[, positive])
ci_rf <- pROC::ci.auc(roc_rf)

roc_svm <- pROC::roc(response  = test$Class, predictor = predict(svmFit,   test, type = "prob")[, positive])
ci_svm <- pROC::ci.auc(roc_svm)

roc_lasso <- pROC::roc(response  = test$Class, predictor = predict(lassoFit, test, type = "prob")[, positive])
ci_lasso  <- pROC::ci.auc(roc_lasso)

ci_df <- data.frame(
  Model = c("Logistic Regression", "Random Forest", "SVM RBF", "Lasso"),
  AUC_Lower = c(ci_lr[1], ci_rf[1], ci_svm[1], ci_lasso[1]),
  AUC = c(ci_lr[2], ci_rf[2], ci_svm[2], ci_lasso[2]),
  AUC_Upper = c(ci_lr[3], ci_rf[3], ci_svm[3], ci_lasso[3]),
  stringsAsFactors = FALSE
)

cat("AUC with 95% CI for each model:\n")
print(ci_df)
```

## Final Model Summary

We selected Random Forest based on the highest ROC AUC of 0.95.This means it is not only highly predictive but also more stable than others.  
Its performance on the test set:

- AUC: [value] (95% CI: 0.8467, 0.95)

While SVM and Lasso also perform well (AUC = 0.91), their lower bounds are weaker. Logistic Regression is the least stable (wide CI) and lowest AUC.

ROC curve is provided:


```{r}
plot(roc_lr, col = "blue", legacy.axes = TRUE, main = "ROC Curves")
plot(roc_rf, col = "green", add = TRUE)
plot(roc_svm, col = "red", add = TRUE)
plot(roc_lasso, col = "purple", add = TRUE)
legend("bottomright", legend = c("Logistic", "Random Forest", "SVM", "Lasso"),
       col = c("blue", "green", "red", "purple"), lwd = 2)

```

```{r}

```







